#!/usr/bin/env python3
"""
Bitrix24 ETL Service - –§–ò–ù–ê–õ–¨–ù–ê–Ø –í–ï–†–°–ò–Ø
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏–∑ Bitrix24 CRM –∏ –∑–∞–≥—Ä—É–∂–∞–µ—Ç –≤ Supabase PostgreSQL
"""

import os
import sys
import time
import logging
from datetime import datetime, timedelta
from typing import Optional, Dict, Any, List
import requests
from supabase import create_client, Client

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('logs/bitrix24_etl.log')
    ]
)
logger = logging.getLogger(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
BITRIX_WEBHOOK = os.getenv('BITRIX_WEBHOOK', '').rstrip('/') + '/'  # –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Å–ª–µ—à –≤ –∫–æ–Ω—Ü–µ
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_KEY')
SYNC_MODE = os.getenv('SYNC_MODE', 'incremental')  # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é incremental
HOURS_BACK = int(os.getenv('HOURS_BACK', '24'))

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
if not all([BITRIX_WEBHOOK, SUPABASE_URL, SUPABASE_KEY]):
    logger.error("‚ùå Missing required environment variables!")
    logger.error(f"   BITRIX_WEBHOOK: {'‚úì' if BITRIX_WEBHOOK else '‚úó'}")
    logger.error(f"   SUPABASE_URL: {'‚úì' if SUPABASE_URL else '‚úó'}")
    logger.error(f"   SUPABASE_KEY: {'‚úì' if SUPABASE_KEY else '‚úó'}")
    sys.exit(1)


class Bitrix24ETL:
    """ETL —Å–µ—Ä–≤–∏—Å –¥–ª—è –≤—ã–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ Bitrix24 –≤ Supabase"""

    def __init__(self):
        self.bitrix_url = BITRIX_WEBHOOK
        self.supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)
        self.rate_limit_delay = 0.6  # –£–≤–µ–ª–∏—á–∏–ª –¥–æ 0.6 –ø–æ—Å–ª–µ 429 –æ—à–∏–±–∫–∏ (Too Many Requests)
        self.created_managers = set()  # –ö—ç—à —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
        self.pending_managers = []  # –ë–∞—Ç—á –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
        self.created_companies = set()  # –ö—ç—à —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π
        self.pending_companies = []  # –ë–∞—Ç—á –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –∫–æ–º–ø–∞–Ω–∏–π
        self.created_contacts = set()  # –ö—ç—à —É–∂–µ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤
        self.pending_contacts = []  # –ë–∞—Ç—á –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –∏–∑ –±–∞–∑—ã –µ—Å–ª–∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ–ª–Ω—É—é –≤—ã–≥—Ä—É–∑–∫—É
        self.load_existing_ids()

    # ==================== –£–¢–ò–õ–ò–¢–´ ====================

    def load_existing_ids(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –∏–∑ Supabase —á—Ç–æ–±—ã –Ω–µ —Å–æ–∑–¥–∞–≤–∞—Ç—å –¥—É–±–ª–∏–∫–∞—Ç—ã"""
        try:
            # –ó–∞–≥—Ä—É–∑–∏—Ç—å ID –∫–æ–º–ø–∞–Ω–∏–π (–ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –í–°–ï)
            logger.info("üìã Loading existing company IDs from Supabase...")
            all_companies = []
            range_start = 0
            range_size = 1000
            while True:
                response = self.supabase.table('companies').select('id').range(range_start, range_start + range_size - 1).execute()
                if not response.data:
                    break
                all_companies.extend(response.data)
                if len(response.data) < range_size:
                    break
                range_start += range_size

            if all_companies:
                self.created_companies = {row['id'] for row in all_companies}
                logger.info(f"  ‚úÖ Loaded {len(self.created_companies)} existing company IDs")

            # –ó–∞–≥—Ä—É–∑–∏—Ç—å ID –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ (–ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ)
            logger.info("üìã Loading existing contact IDs from Supabase...")
            all_contacts = []
            range_start = 0
            while True:
                response = self.supabase.table('contacts').select('id').range(range_start, range_start + range_size - 1).execute()
                if not response.data:
                    break
                all_contacts.extend(response.data)
                if len(response.data) < range_size:
                    break
                range_start += range_size

            if all_contacts:
                self.created_contacts = {row['id'] for row in all_contacts}
                logger.info(f"  ‚úÖ Loaded {len(self.created_contacts)} existing contact IDs")

            # –ó–∞–≥—Ä—É–∑–∏—Ç—å ID –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ (–æ–±—ã—á–Ω–æ –∏—Ö –º–∞–ª–æ, —Ö–≤–∞—Ç–∏—Ç –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞)
            logger.info("üìã Loading existing manager IDs from Supabase...")
            response = self.supabase.table('managers').select('id').execute()
            if response.data:
                self.created_managers = {row['id'] for row in response.data}
                logger.info(f"  ‚úÖ Loaded {len(self.created_managers)} existing manager IDs")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è  Could not load existing IDs from Supabase: {e}")
            logger.warning("  Continuing without cache - may create some duplicate stubs")

    def ensure_manager_exists(self, user_id: int):
        """–î–æ–±–∞–≤–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –≤ –±–∞—Ç—á (—Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–ª–æ–∂–µ–Ω–æ –¥–æ flush)"""
        if not user_id or user_id in self.created_managers:
            return

        manager_data = {
            'id': user_id,
            'name': f'User {user_id}',
            'last_name': None,
            'email': None,
            'work_position': None,
            'personal_phone': None,
            'personal_mobile': None,
            'raw_data': {'ID': user_id, 'note': 'Auto-created on-the-fly'}
        }
        self.pending_managers.append(manager_data)
        self.created_managers.add(user_id)

    def flush_managers(self):
        """–í—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –≤ –±–∞–∑—É"""
        if not self.pending_managers:
            return

        try:
            # –ë–∞—Ç—á–∞–º–∏ –ø–æ 50
            for i in range(0, len(self.pending_managers), 50):
                batch = self.pending_managers[i:i+50]
                self.supabase.table('managers').upsert(batch).execute()
            logger.info(f"  ‚úÖ Flushed {len(self.pending_managers)} managers to DB")
            self.pending_managers = []
        except Exception as e:
            logger.error(f"  ‚ùå Error flushing managers: {e}")
            self.pending_managers = []

    def ensure_company_exists(self, company_id: int):
        """–î–æ–±–∞–≤–∏—Ç—å –∫–æ–º–ø–∞–Ω–∏—é-–∑–∞–≥–ª—É—à–∫—É –≤ –±–∞—Ç—á (–¥–ª—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö –∫–æ–º–ø–∞–Ω–∏–π)"""
        if not company_id or company_id in self.created_companies:
            return

        company_data = {
            'id': company_id,
            'title': f'Company {company_id}',
            'company_type': None,
            'email': None,
            'phone': None,
            'web': None,
            'address': None,
            'date_create': None,
            'date_modify': None,
            'assigned_by_id': None,
            'created_by_id': None,
            'raw_data': {'ID': company_id, 'note': 'Auto-created stub for missing company'}
        }
        self.pending_companies.append(company_data)
        self.created_companies.add(company_id)

    def flush_companies(self):
        """–í—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–π-–∑–∞–≥–ª—É—à–µ–∫ –≤ –±–∞–∑—É"""
        if not self.pending_companies:
            return

        try:
            # –ë–∞—Ç—á–∞–º–∏ –ø–æ 50
            for i in range(0, len(self.pending_companies), 50):
                batch = self.pending_companies[i:i+50]
                self.supabase.table('companies').upsert(batch).execute()
            logger.info(f"  ‚úÖ Flushed {len(self.pending_companies)} company stubs to DB")
            self.pending_companies = []
        except Exception as e:
            logger.error(f"  ‚ùå Error flushing companies: {e}")
            self.pending_companies = []

    def ensure_contact_exists(self, contact_id: int):
        """–î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–∞–∫—Ç-–∑–∞–≥–ª—É—à–∫—É –≤ –±–∞—Ç—á (—Å–æ–∑–¥–∞–Ω–∏–µ –æ—Ç–ª–æ–∂–µ–Ω–æ –¥–æ flush)"""
        if not contact_id or contact_id in self.created_contacts:
            return

        contact_data = {
            'id': contact_id,
            'name': f'Contact {contact_id}',
            'last_name': None,
            'email': None,
            'phone': None,
            'post': None,
            'birthdate': None,
            'date_create': None,
            'date_modify': None,
            'company_id': None,
            'assigned_by_id': None,
            'created_by_id': None,
            'source_id': None,
            'source_description': None,
            'raw_data': {'ID': contact_id, 'note': 'Auto-created stub for missing contact'}
        }
        self.pending_contacts.append(contact_data)
        self.created_contacts.add(contact_id)

    def flush_contacts(self):
        """–í—Å—Ç–∞–≤–∏—Ç—å –≤—Å–µ—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤-–∑–∞–≥–ª—É—à–µ–∫ –≤ –±–∞–∑—É"""
        if not self.pending_contacts:
            return

        try:
            # –ë–∞—Ç—á–∞–º–∏ –ø–æ 50
            for i in range(0, len(self.pending_contacts), 50):
                batch = self.pending_contacts[i:i+50]
                self.supabase.table('contacts').upsert(batch).execute()
            logger.info(f"  ‚úÖ Flushed {len(self.pending_contacts)} contact stubs to DB")
            self.pending_contacts = []
        except Exception as e:
            logger.error(f"  ‚ùå Error flushing contacts: {e}")
            self.pending_contacts = []

    @staticmethod
    def safe_int(value: Any, default: Optional[int] = None) -> Optional[int]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ int"""
        if value is None or value == '' or value == 'null':
            return default
        try:
            return int(float(value))
        except (ValueError, TypeError):
            return default
    
    @staticmethod
    def safe_float(value: Any, default: Optional[float] = None) -> Optional[float]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ float"""
        if value is None or value == '' or value == 'null':
            return default
        try:
            return float(value)
        except (ValueError, TypeError):
            return default
    
    @staticmethod
    def safe_datetime(value: Any) -> Optional[str]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –¥–∞—Ç—ã –≤ ISO —Ñ–æ—Ä–º–∞—Ç"""
        if not value or value == '' or value == 'null':
            return None
        try:
            if isinstance(value, str):
                value = value.replace('Z', '+00:00')
                dt = datetime.fromisoformat(value)
                return dt.isoformat()
            return None
        except (ValueError, TypeError):
            return None
    
    @staticmethod
    def safe_bool(value: Any) -> bool:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ bool"""
        if value is None or value == '':
            return False
        if isinstance(value, bool):
            return value
        if isinstance(value, str):
            return value.upper() in ('Y', 'YES', 'TRUE', '1')
        return bool(value)
    
    def bitrix_request(self, method: str, params: Optional[Dict] = None) -> List[Dict]:
        """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å –∫ Bitrix24 API —Å –ø–∞–≥–∏–Ω–∞—Ü–∏–µ–π"""
        all_results = []
        start = 0
        iterations = 0
        max_iterations = 1000  # –ó–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
        last_count = 0
        stuck_counter = 0  # –°—á—ë—Ç—á–∏–∫ "–∑–∞—Å—Ç—Ä–µ–≤–∞–Ω–∏–π" –Ω–∞ –æ–¥–Ω–æ–º –º–µ—Å—Ç–µ

        if params is None:
            params = {}

        # –õ–æ–≥–∏—Ä—É–µ–º –Ω–∞—á–∞–ª–æ –∑–∞–ø—Ä–æ—Å–∞
        logger.info(f"  üîÑ Starting Bitrix24 request: {method}")

        while True:
            iterations += 1
            if iterations > max_iterations:
                logger.error(f"  ‚ùå Max iterations ({max_iterations}) reached! Breaking loop.")
                break

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∑–∞–≤–∏—Å–∞–Ω–∏–µ (–µ—Å–ª–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–µ —Ä–∞—Å—Ç—ë—Ç)
            if len(all_results) == last_count:
                stuck_counter += 1
                if stuck_counter > 3:
                    logger.error(f"  ‚ùå Stuck at {len(all_results)} records for 3 iterations! Breaking.")
                    break
            else:
                stuck_counter = 0
                last_count = len(all_results)
            request_params = {**params, 'start': start}
            url = f"{self.bitrix_url}{method}.json"

            try:
                time.sleep(self.rate_limit_delay)
                response = requests.get(url, params=request_params, timeout=30)

                # –û–±—Ä–∞–±–æ—Ç–∫–∞ 429 Too Many Requests
                if response.status_code == 429:
                    wait_time = 60  # –ñ–¥—ë–º 60 —Å–µ–∫—É–Ω–¥
                    logger.warning(f"‚ö†Ô∏è  429 Too Many Requests! Waiting {wait_time}s before retry...")
                    time.sleep(wait_time)
                    continue  # –ü–æ–≤—Ç–æ—Ä—è–µ–º —Ç–æ—Ç –∂–µ –∑–∞–ø—Ä–æ—Å

                response.raise_for_status()
                data = response.json()

                if 'result' not in data:
                    break

                results = data['result']
                if not results:
                    break

                # –ï—Å–ª–∏ result - —ç—Ç–æ dict (–¥–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –º–µ—Ç–æ–¥–æ–≤ —Ç–∏–ø–∞ crm.category.list),
                # –∏–∑–≤–ª–µ–∫–∞–µ–º –º–∞—Å—Å–∏–≤ –∏–∑ –≤–ª–æ–∂–µ–Ω–Ω–æ–≥–æ –∫–ª—é—á–∞ –∏–ª–∏ –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ –º–∞—Å—Å–∏–≤
                if isinstance(results, dict):
                    # –î–ª—è crm.category.list —Ä–µ–∑—É–ª—å—Ç–∞—Ç –º–æ–∂–µ—Ç –±—ã—Ç—å {'categories': [...]}
                    # –∏–ª–∏ {'0': {...}, '1': {...}} - –≤ —ç—Ç–æ–º —Å–ª—É—á–∞–µ –±–µ—Ä–µ–º values
                    if 'categories' in results:
                        results = results['categories']
                    else:
                        # –ï—Å–ª–∏ dict —Å —á–∏—Å–ª–æ–≤—ã–º–∏ –∫–ª—é—á–∞–º–∏, –±–µ—Ä–µ–º values
                        results = list(results.values())

                # –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤—Å—ë –µ—â—ë dict –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏, –æ–±–æ—Ä–∞—á–∏–≤–∞–µ–º –≤ —Å–ø–∏—Å–æ–∫
                if isinstance(results, dict):
                    results = [results]

                all_results.extend(results)

                # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–∞–∂–¥—ã–µ 500 –∑–∞–ø–∏—Å–µ–π
                if len(all_results) % 500 == 0:
                    total = data.get('total', 0)
                    logger.info(f"  ‚è≥ {method}: loaded {len(all_results)}/{total} records...")

                total = data.get('total', 0)

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –≤—ã—Ö–æ–¥–∞ –∏–∑ —Ü–∏–∫–ª–∞
                if len(results) < 50:
                    logger.info(f"  üõë Got less than 50 results ({len(results)}), stopping pagination")
                    break

                if total > 0 and len(all_results) >= total:
                    logger.info(f"  üõë Loaded all {total} records, stopping pagination")
                    break

                # –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ø—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∏–ª–∏ —É–∂–µ –º–Ω–æ–≥–æ –Ω–æ total=0 –∏–ª–∏ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–µ–Ω ‚Üí BREAK
                if len(all_results) >= 2000 and (total == 0 or total < len(all_results)):
                    logger.error(f"  ‚ùå Loaded {len(all_results)} but total={total} (invalid)! Breaking to prevent infinite loop.")
                    break

                # EMERGENCY: –ï—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∏–ª–∏ >= 50000 –õ–Æ–ë–´–• –∑–∞–ø–∏—Å–µ–π ‚Üí FORCE BREAK!
                if len(all_results) >= 50000:
                    logger.warning(f"  ‚ö†Ô∏è  EMERGENCY BREAK at {len(all_results)} records! Returning what we have to avoid infinite loop.")
                    break

                start += 50

            except requests.exceptions.HTTPError as e:
                if '429' in str(e):
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ 429 –µ—Å–ª–∏ –Ω–µ –ø–æ–π–º–∞–ª–∏ –≤—ã—à–µ
                    logger.warning(f"‚ö†Ô∏è  429 in exception! Waiting 60s...")
                    time.sleep(60)
                    continue
                logger.error(f"‚ùå HTTP Error in Bitrix24 request {method}: {e}")
                break
            except Exception as e:
                logger.error(f"‚ùå Error in Bitrix24 request {method}: {e}")
                break

        logger.info(f"  ‚úÖ {method}: completed, total {len(all_results)} records")
        return all_results
    
    def log_sync_start(self, entity_type: str) -> int:
        """–ó–∞–ø–∏—Å–∞—Ç—å –Ω–∞—á–∞–ª–æ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
        try:
            result = self.supabase.table('sync_log').insert({
                'sync_type': SYNC_MODE,
                'entity_type': entity_type,
                'status': 'running',
                'started_at': datetime.utcnow().isoformat(),
                'records_processed': 0
            }).execute()
            return result.data[0]['id']
        except Exception as e:
            logger.error(f"‚ùå Error logging sync start: {e}")
            return 0
    
    def log_sync_end(self, sync_id: int, status: str, records: int, error_msg: Optional[str] = None):
        """–ó–∞–ø–∏—Å–∞—Ç—å –æ–∫–æ–Ω—á–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏"""
        try:
            self.supabase.table('sync_log').update({
                'status': status,
                'finished_at': datetime.utcnow().isoformat(),
                'records_processed': records,
                'error_message': error_msg
            }).eq('id', sync_id).execute()
        except Exception as e:
            logger.error(f"‚ùå Error logging sync end: {e}")
    
    # ==================== –ò–ó–í–õ–ï–ß–ï–ù–ò–ï –î–ê–ù–ù–´–• ====================
    
    
    # ==================== –°–ü–†–ê–í–û–ß–ù–ò–ö–ò ====================

    def extract_dictionaries(self):
        """–ó–∞–≥—Ä—É–∑–∏—Ç—å –≤—Å–µ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏: –≤–æ—Ä–æ–Ω–∫–∏, —Å—Ç–∞–¥–∏–∏, —Å—Ç–∞—Ç—É—Å—ã"""
        logger.info("üìö Extracting dictionaries...")

        try:
            # 1. –í–æ—Ä–æ–Ω–∫–∏ —Å–¥–µ–ª–æ–∫ (categories)
            logger.info("  üìã Loading deal categories...")
            categories = self.bitrix_request('crm.category.list', {'entityTypeId': 2})  # 2 = DEAL

            if categories:
                for cat in categories:
                    # –ï—Å–ª–∏ cat - —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ –Ω–µ dict, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                    if not isinstance(cat, dict):
                        logger.warning(f"  ‚ö†Ô∏è  Skipping non-dict category: {type(cat)} = {cat}")
                        continue

                    cat_data = {
                        'id': self.safe_int(cat.get('id')),
                        'name': cat.get('name') or f"Category {cat.get('id')}",
                        'sort': self.safe_int(cat.get('sort')) or 500,
                        'is_default': cat.get('isDefault') == 'Y'
                    }
                    self.supabase.table('deal_categories').upsert(cat_data).execute()
                logger.info(f"  ‚úÖ Loaded {len(categories)} deal categories")

            # 2. –°—Ç–∞–¥–∏–∏ —Å–¥–µ–ª–æ–∫ (stages)
            logger.info("  üìã Loading deal stages...")
            all_stages = []
            # –ü–æ–ª—É—á–∞–µ–º —Å—Ç–∞–¥–∏–∏ –¥–ª—è –∫–∞–∂–¥–æ–π –≤–æ—Ä–æ–Ω–∫–∏
            if categories:
                for cat in categories:
                    cat_id = self.safe_int(cat.get('id'))
                    stages = self.bitrix_request('crm.status.list', {
                        'filter': {'ENTITY_ID': f'DEAL_STAGE_{cat_id}'}
                    })
                    for stage in stages:
                        stage_data = {
                            'id': stage['STATUS_ID'],
                            'name': stage.get('NAME') or stage['STATUS_ID'],
                            'category_id': cat_id,
                            'status_id': stage['STATUS_ID'],
                            'sort': self.safe_int(stage.get('SORT')) or 500,
                            'color': stage.get('COLOR'),
                            'semantics': stage.get('SEMANTICS')
                        }
                        all_stages.append(stage_data)

            if all_stages:
                # –í—Å—Ç–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏
                for i in range(0, len(all_stages), 50):
                    batch = all_stages[i:i+50]
                    self.supabase.table('deal_stages').upsert(batch).execute()
                logger.info(f"  ‚úÖ Loaded {len(all_stages)} deal stages")

            # 3. –°—Ç–∞—Ç—É—Å—ã –ª–∏–¥–æ–≤
            logger.info("  üìã Loading lead statuses...")
            lead_statuses = self.bitrix_request('crm.status.list', {
                'filter': {'ENTITY_ID': 'STATUS'}
            })
            if lead_statuses:
                status_batch = []
                for status in lead_statuses:
                    status_data = {
                        'id': status['STATUS_ID'],
                        'name': status.get('NAME') or status['STATUS_ID'],
                        'sort': self.safe_int(status.get('SORT')) or 500,
                        'color': status.get('COLOR'),
                        'semantics': status.get('SEMANTICS')
                    }
                    status_batch.append(status_data)

                for i in range(0, len(status_batch), 50):
                    batch = status_batch[i:i+50]
                    self.supabase.table('lead_statuses').upsert(batch).execute()
                logger.info(f"  ‚úÖ Loaded {len(status_batch)} lead statuses")

            logger.info("  ‚úÖ Dictionaries loaded successfully")

        except Exception as e:
            logger.error(f"  ‚ùå Error loading dictionaries: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –µ—Å–ª–∏ —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∏—Å—å

    # ==================== –û–°–ù–û–í–ù–´–ï –°–£–©–ù–û–°–¢–ò ====================

    def extract_companies(self) -> int:
        """–ò–∑–≤–ª–µ—á—å –∫–æ–º–ø–∞–Ω–∏–∏"""
        logger.info("üì• Extracting companies...")
        sync_id = self.log_sync_start('companies')

        processed = 0
        try:
            params = {}

            if SYNC_MODE == 'incremental':
                cutoff_time = (datetime.utcnow() - timedelta(hours=HOURS_BACK)).isoformat()
                params['filter'] = {'>DATE_MODIFY': cutoff_time}

            companies = self.bitrix_request('crm.company.list', params)

            batch = []

            for company in companies:
                company_id = self.safe_int(company['ID'])

                # –û—Ç–º–µ—Ç–∏—Ç—å —á—Ç–æ —ç—Ç–∞ –∫–æ–º–ø–∞–Ω–∏—è —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ (—Ä–µ–∞–ª—å–Ω–∞—è, –Ω–µ –∑–∞–≥–ª—É—à–∫–∞)
                self.created_companies.add(company_id)

                # –°–æ–∑–¥–∞—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –±–∞–∑–µ
                assigned_by_id = self.safe_int(company.get('ASSIGNED_BY_ID'))
                created_by_id = self.safe_int(company.get('CREATED_BY_ID'))
                if assigned_by_id:
                    self.ensure_manager_exists(assigned_by_id)
                if created_by_id:
                    self.ensure_manager_exists(created_by_id)

                # EMAIL –∏ PHONE –ø—Ä–∏—Ö–æ–¥—è—Ç –∫–∞–∫ –º–∞—Å—Å–∏–≤—ã
                email_value = None
                if company.get('EMAIL') and isinstance(company['EMAIL'], list) and len(company['EMAIL']) > 0:
                    email_value = company['EMAIL'][0].get('VALUE')

                phone_value = None
                if company.get('PHONE') and isinstance(company['PHONE'], list) and len(company['PHONE']) > 0:
                    phone_value = company['PHONE'][0].get('VALUE')

                company_data = {
                    'id': company_id,
                    'title': company.get('TITLE') or None,
                    'company_type': company.get('COMPANY_TYPE') or None,
                    'email': email_value,
                    'phone': phone_value,
                    'web': company.get('WEB') or None,
                    'address': company.get('ADDRESS') or None,
                    'date_create': self.safe_datetime(company.get('DATE_CREATE')),
                    'date_modify': self.safe_datetime(company.get('DATE_MODIFY')),
                    'assigned_by_id': assigned_by_id if assigned_by_id else None,
                    'created_by_id': created_by_id if created_by_id else None,
                    'raw_data': company
                }

                batch.append(company_data)
                processed += 1

                if len(batch) >= 50:
                    self.supabase.table('companies').upsert(batch).execute()
                    logger.info(f"  üìä Companies extracted: {processed}")
                    batch = []

            # –í—Å—Ç–∞–≤–∏—Ç—å –æ—Å—Ç–∞—Ç–æ–∫
            if batch:
                self.supabase.table('companies').upsert(batch).execute()

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
            self.flush_managers()

            logger.info(f"  ‚úÖ Companies extracted: {processed}")
            self.log_sync_end(sync_id, 'completed', processed)
            return processed

        except Exception as e:
            logger.error(f"  ‚ùå Error extracting companies: {e}")
            self.flush_managers()  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.log_sync_end(sync_id, 'failed', processed)
            return processed

    def extract_contacts(self) -> int:
        """–ò–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–∞–∫—Ç—ã"""
        logger.info("üì• Extracting contacts...")
        sync_id = self.log_sync_start('contacts')
        
        processed = 0
        try:
            params = {}
            
            if SYNC_MODE == 'incremental':
                cutoff_time = (datetime.utcnow() - timedelta(hours=HOURS_BACK)).isoformat()
                params['filter'] = {'>DATE_MODIFY': cutoff_time}
            
            contacts = self.bitrix_request('crm.contact.list', params)
            
            batch = []
            
            for contact in contacts:
                # –°–æ–∑–¥–∞—Ç—å –∫–æ–º–ø–∞–Ω–∏—é-–∑–∞–≥–ª—É—à–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç –≤ –±–∞–∑–µ
                company_id = self.safe_int(contact.get('COMPANY_ID'))
                if company_id:
                    self.ensure_company_exists(company_id)

                # –°–æ–∑–¥–∞—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
                assigned_by_id = self.safe_int(contact.get('ASSIGNED_BY_ID'))
                created_by_id = self.safe_int(contact.get('CREATED_BY_ID'))
                if assigned_by_id:
                    self.ensure_manager_exists(assigned_by_id)
                if created_by_id:
                    self.ensure_manager_exists(created_by_id)

                # EMAIL –∏ PHONE –ø—Ä–∏—Ö–æ–¥—è—Ç –∫–∞–∫ –º–∞—Å—Å–∏–≤—ã
                email_value = None
                if contact.get('EMAIL') and isinstance(contact['EMAIL'], list) and len(contact['EMAIL']) > 0:
                    email_value = contact['EMAIL'][0].get('VALUE')

                phone_value = None
                if contact.get('PHONE') and isinstance(contact['PHONE'], list) and len(contact['PHONE']) > 0:
                    phone_value = contact['PHONE'][0].get('VALUE')

                # –°–æ–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω–æ–µ –∏–º—è
                name_parts = [
                    contact.get('NAME'),
                    contact.get('SECOND_NAME'),
                    contact.get('LAST_NAME')
                ]
                full_name = ' '.join(filter(None, name_parts)) or None

                contact_data = {
                    'id': self.safe_int(contact['ID']),
                    'name': contact.get('NAME') or None,
                    'last_name': contact.get('LAST_NAME') or None,
                    'second_name': contact.get('SECOND_NAME') or None,
                    'full_name': full_name,
                    'email': email_value,
                    'phone': phone_value,
                    'post': contact.get('POST') or None,
                    'birthdate': self.safe_datetime(contact.get('BIRTHDATE')),
                    'date_create': self.safe_datetime(contact.get('DATE_CREATE')),
                    'date_modify': self.safe_datetime(contact.get('DATE_MODIFY')),
                    'company_id': company_id,
                    'assigned_by_id': self.safe_int(contact.get('ASSIGNED_BY_ID')),
                    'created_by_id': self.safe_int(contact.get('CREATED_BY_ID')),
                    'source_id': contact.get('SOURCE_ID') or None,
                    'source_description': contact.get('SOURCE_DESCRIPTION') or None,
                    'raw_data': contact
                }
                
                batch.append(contact_data)
                processed += 1

                if len(batch) >= 50:
                    # –§–ª–∞—à–∏–º –∑–∞–≥–ª—É—à–∫–∏ –ü–ï–†–ï–î –≤—Å—Ç–∞–≤–∫–æ–π –±–∞—Ç—á–∞
                    self.flush_companies()
                    self.flush_managers()
                    self.supabase.table('contacts').upsert(batch).execute()
                    logger.info(f"  üìä Contacts extracted: {processed}")
                    batch = []

            if batch:
                # –§–ª–∞—à–∏–º –∑–∞–≥–ª—É—à–∫–∏ –ü–ï–†–ï–î –≤—Å—Ç–∞–≤–∫–æ–π –æ—Å—Ç–∞—Ç–∫–∞
                self.flush_companies()
                self.flush_managers()
                self.supabase.table('contacts').upsert(batch).execute()

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏
            self.flush_companies()
            self.flush_managers()

            logger.info(f"  ‚úÖ Contacts extracted: {processed}")
            self.log_sync_end(sync_id, 'completed', processed)
            return processed

        except Exception as e:
            logger.error(f"  ‚ùå Error extracting contacts: {e}")
            self.flush_companies()  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∑–∞–≥–ª—É—à–∫–∏ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.flush_managers()
            self.log_sync_end(sync_id, 'failed', processed, str(e))
            return processed
    
    def extract_deals(self) -> int:
        """–ò–∑–≤–ª–µ—á—å —Å–¥–µ–ª–∫–∏ (streaming - –≤—Å—Ç–∞–≤–∫–∞ –≤–æ –≤—Ä–µ–º—è –∑–∞–≥—Ä—É–∑–∫–∏)"""
        logger.info("üì• Extracting deals...")
        sync_id = self.log_sync_start('deals')

        processed = 0
        batch = []

        try:
            params = {}

            if SYNC_MODE == 'incremental':
                cutoff_time = (datetime.utcnow() - timedelta(hours=HOURS_BACK)).isoformat()
                params['filter'] = {'>DATE_MODIFY': cutoff_time}

            logger.info("  üîÑ Starting streaming deals extraction...")

            # STREAMING: –≥—Ä—É–∑–∏–º –∏ –≤—Å—Ç–∞–≤–ª—è–µ–º –ø–æ—Å—Ç—Ä–∞–Ω–∏—á–Ω–æ, –ë–ï–ó –Ω–∞–∫–æ–ø–ª–µ–Ω–∏—è –≤ –ø–∞–º—è—Ç—å
            start = 0
            page = 0

            while True:
                page += 1
                request_params = {**params, 'start': start}
                url = f"{self.bitrix_url}crm.deal.list.json"

                time.sleep(self.rate_limit_delay)
                response = requests.get(url, params=request_params, timeout=30)
                response.raise_for_status()
                data = response.json()

                if 'result' not in data or not data['result']:
                    break

                deals_page = data['result']
                logger.info(f"  üìÑ Page {page}: processing {len(deals_page)} deals...")

                for deal in deals_page:
                    # –°–æ–∑–¥–∞—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –±–∞–∑–µ
                    self.ensure_manager_exists(self.safe_int(deal.get('ASSIGNED_BY_ID')))
                    self.ensure_manager_exists(self.safe_int(deal.get('CREATED_BY_ID')))
                    self.ensure_manager_exists(self.safe_int(deal.get('MODIFY_BY_ID')))

                    # –°–æ–∑–¥–∞—Ç—å –∫–æ–º–ø–∞–Ω–∏–∏/–∫–æ–Ω—Ç–∞–∫—Ç—ã –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –±–∞–∑–µ
                    company_id = self.safe_int(deal.get('COMPANY_ID'))
                    contact_id = self.safe_int(deal.get('CONTACT_ID'))
                    if company_id:
                        self.ensure_company_exists(company_id)
                    if contact_id:
                        self.ensure_contact_exists(contact_id)

                    deal_data = {
                        'id': self.safe_int(deal['ID']),
                        'title': deal.get('TITLE') or None,
                        'type_id': deal.get('TYPE_ID') or None,
                        'category_id': self.safe_int(deal.get('CATEGORY_ID')),
                        'stage_id': deal.get('STAGE_ID') or None,
                        'stage_semantic_id': deal.get('STAGE_SEMANTIC_ID') or None,
                        'opportunity': self.safe_float(deal.get('OPPORTUNITY')),
                        'currency_id': deal.get('CURRENCY_ID') or 'RUB',
                        'tax_value': self.safe_float(deal.get('TAX_VALUE')),
                        'company_id': company_id if company_id else None,
                        'contact_id': contact_id if contact_id else None,
                        'assigned_by_id': self.safe_int(deal.get('ASSIGNED_BY_ID')) or None,
                        'created_by_id': self.safe_int(deal.get('CREATED_BY_ID')) or None,
                        'closed': self.safe_bool(deal.get('CLOSED')),
                        'begindate': self.safe_datetime(deal.get('BEGINDATE')),
                        'closedate': self.safe_datetime(deal.get('CLOSEDATE')),
                        'date_create': self.safe_datetime(deal.get('DATE_CREATE')),
                        'date_modify': self.safe_datetime(deal.get('DATE_MODIFY')),
                        'utm_source': deal.get('UTM_SOURCE') or None,
                        'utm_medium': deal.get('UTM_MEDIUM') or None,
                        'utm_campaign': deal.get('UTM_CAMPAIGN') or None,
                        'utm_content': deal.get('UTM_CONTENT') or None,
                        'utm_term': deal.get('UTM_TERM') or None,
                        'source_id': deal.get('SOURCE_ID') or None,
                        'source_description': deal.get('SOURCE_DESCRIPTION') or None,
                        'raw_data': deal
                    }

                    batch.append(deal_data)
                    processed += 1

                # –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö deals –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ - –ø—Ä–æ–≤–µ—Ä—è–µ–º –±–∞—Ç—á
                if len(batch) >= 50:
                    # –§–ª–∞—à–∏–º –∑–∞–≥–ª—É—à–∫–∏ –ü–ï–†–ï–î –≤—Å—Ç–∞–≤–∫–æ–π –±–∞—Ç—á–∞
                    self.flush_companies()
                    self.flush_contacts()
                    self.flush_managers()
                    try:
                        response = self.supabase.table('deals').upsert(batch).execute()
                        logger.info(f"  üìä Deals extracted: {processed}, inserted: {len(response.data) if response.data else 0}")
                    except Exception as e:
                        logger.error(f"  ‚ùå Error upserting deals batch: {e}")
                        logger.error(f"  Sample deal data: {batch[0] if batch else 'empty'}")
                        raise
                    batch = []

                # –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ª–æ–≤–∏–π –≤—ã—Ö–æ–¥–∞
                if len(deals_page) < 50:
                    logger.info(f"  üõë Got less than 50 results ({len(deals_page)}), stopping pagination")
                    break

                total = data.get('total', 0)
                if total > 0 and processed >= total:
                    logger.info(f"  üõë Loaded all {total} records, stopping pagination")
                    break

                # EMERGENCY: –∑–∞—â–∏—Ç–∞ –æ—Ç –±–µ—Å–∫–æ–Ω–µ—á–Ω–æ–≥–æ —Ü–∏–∫–ª–∞
                if processed >= 50000:
                    logger.warning(f"  ‚ö†Ô∏è  EMERGENCY BREAK at {processed} records!")
                    break

                start += 50

            if batch:
                # –§–ª–∞—à–∏–º –∑–∞–≥–ª—É—à–∫–∏ –ü–ï–†–ï–î –≤—Å—Ç–∞–≤–∫–æ–π –æ—Å—Ç–∞—Ç–∫–∞
                self.flush_companies()
                self.flush_contacts()
                self.flush_managers()
                try:
                    response = self.supabase.table('deals').upsert(batch).execute()
                    logger.info(f"  ‚úÖ Final batch inserted: {len(response.data) if response.data else 0} deals")
                except Exception as e:
                    logger.error(f"  ‚ùå Error upserting final deals batch: {e}")
                    logger.error(f"  Sample deal data: {batch[0] if batch else 'empty'}")
                    raise

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã–µ –∑–∞–≥–ª—É—à–∫–∏
            self.flush_companies()
            self.flush_contacts()
            self.flush_managers()

            logger.info(f"  ‚úÖ Deals extracted: {processed}")
            self.log_sync_end(sync_id, 'completed', processed)
            return processed

        except Exception as e:
            logger.error(f"  ‚ùå Error extracting deals: {e}")
            self.flush_managers()  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.log_sync_end(sync_id, 'failed', processed, str(e))
            return processed
    
    def extract_activities(self) -> int:
        """–ò–∑–≤–ª–µ—á—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ (–∑–≤–æ–Ω–∫–∏, –≤—Å—Ç—Ä–µ—á–∏, email)"""
        logger.info("üì• Extracting activities...")
        sync_id = self.log_sync_start('activities')
        
        processed = 0
        try:
            params = {}
            
            if SYNC_MODE == 'incremental':
                cutoff_time = (datetime.utcnow() - timedelta(hours=HOURS_BACK)).isoformat()
                params['filter'] = {'>LAST_UPDATED': cutoff_time}
            
            activities = self.bitrix_request('crm.activity.list', params)
            
            batch = []
            
            for activity in activities:
                # –°–æ–∑–¥–∞—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ –±–∞–∑–µ
                self.ensure_manager_exists(self.safe_int(activity.get('RESPONSIBLE_ID')))
                self.ensure_manager_exists(self.safe_int(activity.get('AUTHOR_ID')))
                self.ensure_manager_exists(self.safe_int(activity.get('EDITOR_ID')))

                # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–≤–æ–Ω–∫–∞
                call_duration = None
                if activity.get('PROVIDER_ID') == 'VOXIMPLANT':
                    call_duration = self.safe_int(activity.get('RESULT_VALUE'))

                activity_data = {
                    'id': self.safe_int(activity['ID']),
                    'owner_id': self.safe_int(activity.get('OWNER_ID')),
                    'owner_type_id': self.safe_int(activity.get('OWNER_TYPE_ID')),
                    'type_id': self.safe_int(activity.get('TYPE_ID')),
                    'provider_id': activity.get('PROVIDER_ID') or None,
                    'provider_type_id': activity.get('PROVIDER_TYPE_ID') or None,
                    'subject': activity.get('SUBJECT') or None,
                    'description': activity.get('DESCRIPTION') or None,
                    'description_type': activity.get('DESCRIPTION_TYPE') or None,
                    'direction': self.safe_int(activity.get('DIRECTION')),
                    'priority': self.safe_int(activity.get('PRIORITY')),
                    'status': self.safe_int(activity.get('STATUS')),
                    'completed': self.safe_datetime(activity.get('COMPLETED')),
                    'start_time': self.safe_datetime(activity.get('START_TIME')),
                    'end_time': self.safe_datetime(activity.get('END_TIME')),
                    'deadline': self.safe_datetime(activity.get('DEADLINE')),
                    'created': self.safe_datetime(activity.get('CREATED')),
                    'last_updated': self.safe_datetime(activity.get('LAST_UPDATED')),
                    'responsible_id': self.safe_int(activity.get('RESPONSIBLE_ID')),
                    'author_id': self.safe_int(activity.get('AUTHOR_ID')),
                    'call_duration': call_duration,
                    'raw_data': activity
                }
                
                batch.append(activity_data)
                processed += 1
                
                if len(batch) >= 50:
                    self.supabase.table('activities').upsert(batch).execute()
                    logger.info(f"  üìä Activities extracted: {processed}")
                    batch = []
            
            if batch:
                self.supabase.table('activities').upsert(batch).execute()

            # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤—Å–µ—Ö –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤
            self.flush_managers()

            logger.info(f"  ‚úÖ Activities extracted: {processed}")
            self.log_sync_end(sync_id, 'completed', processed)
            return processed

        except Exception as e:
            logger.error(f"  ‚ùå Error extracting activities: {e}")
            self.flush_managers()  # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –º–µ–Ω–µ–¥–∂–µ—Ä–æ–≤ –¥–∞–∂–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ
            self.log_sync_end(sync_id, 'failed', processed, str(e))
            return processed
    
    # ==================== –†–ê–°–ß–Å–¢ –ü–ê–¢–¢–ï–†–ù–û–í ====================
    
    def calculate_patterns(self):
        """–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è —Å–¥–µ–ª–æ–∫"""
        logger.info("üîÑ Calculating deal patterns...")
        
        try:
            # –ü—Ä—è–º–æ–π SQL —á–µ—Ä–µ–∑ postgrest
            # –í–º–µ—Å—Ç–æ RPC –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä—è–º—ã–µ UPDATE/INSERT
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–¥–µ–ª–æ–∫
            deals_response = self.supabase.table('deals').select('id').execute()
            deal_ids = [d['id'] for d in deals_response.data]
            
            if not deal_ids:
                logger.info("  ‚ÑπÔ∏è No deals to calculate patterns")
                return
            
            # –î–ª—è –∫–∞–∂–¥–æ–π —Å–¥–µ–ª–∫–∏ —Å—á–∏—Ç–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            for deal_id in deal_ids:
                # –ü–æ–ª—É—á–∞–µ–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Å–¥–µ–ª–∫–∏
                activities = self.supabase.table('activities')\
                    .select('*')\
                    .eq('owner_id', deal_id)\
                    .eq('owner_type_id', 2)\
                    .execute()
                
                if not activities.data:
                    continue
                
                # –°—á–∏—Ç–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
                touches_count = len(activities.data)
                calls_count = len([a for a in activities.data if a.get('type_id') == 2])
                emails_count = len([a for a in activities.data if a.get('type_id') == 4])
                meetings_count = len([a for a in activities.data if a.get('type_id') == 1])
                
                call_durations = [a.get('call_duration') for a in activities.data if a.get('call_duration')]
                avg_call_duration = sum(call_durations) / len(call_durations) if call_durations else None
                
                created_dates = [a.get('created') for a in activities.data if a.get('created')]
                first_activity_date = min(created_dates) if created_dates else None
                last_activity_date = max(created_dates) if created_dates else None
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω
                pattern_data = {
                    'deal_id': deal_id,
                    'touches_count': touches_count,
                    'calls_count': calls_count,
                    'emails_count': emails_count,
                    'meetings_count': meetings_count,
                    'avg_call_duration': avg_call_duration,
                    'first_activity_date': first_activity_date,
                    'last_activity_date': last_activity_date
                }
                
                self.supabase.table('deal_patterns').upsert(pattern_data).execute()
            
            logger.info(f"  ‚úÖ Patterns calculated for {len(deal_ids)} deals")
            
        except Exception as e:
            logger.error(f"  ‚ùå Error calculating patterns: {e}")
    
    # ==================== –û–°–ù–û–í–ù–´–ï –ú–ï–¢–û–î–´ ====================
    
    def full_sync(self):
        """
        –ü–æ–ª–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
        –ú–µ–Ω–µ–¥–∂–µ—Ä—ã —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ on-the-fly –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ companies/deals/activities
        """
        logger.info("=" * 80)
        logger.info("üîÑ FULL SYNC STARTED")
        logger.info("=" * 80)

        start_time = time.time()

        # –ó–∞–≥—Ä—É–∑–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ (–≤–æ—Ä–æ–Ω–∫–∏, —Å—Ç–∞–¥–∏–∏, —Å—Ç–∞—Ç—É—Å—ã)
        self.extract_dictionaries()

        # Managers —Å–æ–∑–¥–∞—é—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –∑–∞–≥—Ä—É–∑–∫–∏
        companies_count = self.extract_companies()
        contacts_count = self.extract_contacts()
        deals_count = self.extract_deals()
        activities_count = self.extract_activities()
        managers_count = len(self.created_managers)

        # –û–¢–ö–õ–Æ–ß–ï–ù–û: calculate_patterns() —Ç–æ—Ä–º–æ–∑–∏—Ç –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Å–¥–µ–ª–æ–∫
        # self.calculate_patterns()
        
        duration = time.time() - start_time
        
        logger.info("=" * 80)
        logger.info("‚úÖ FULL SYNC COMPLETED")
        logger.info(f"   Duration: {duration:.2f}s")
        logger.info(f"   Managers: {managers_count}")
        logger.info(f"   Companies: {companies_count}")
        logger.info(f"   Contacts: {contacts_count}")
        logger.info(f"   Deals: {deals_count}")
        logger.info(f"   Activities: {activities_count}")
        logger.info("=" * 80)
    
    def incremental_sync(self):
        """
        –£–ú–ù–ê–Ø –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è:
        1. –û–±–Ω–æ–≤–ª—è–µ—Ç —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏
        2. –û–±–æ–≥–∞—â–∞–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–º–∏ –ø–æ–ª—è–º–∏
        3. –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø–∏—Å–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ HOURS_BACK —á–∞—Å–æ–≤
        """
        logger.info("=" * 80)
        logger.info(f"üîÑ SMART INCREMENTAL SYNC STARTED (last {HOURS_BACK}h)")
        logger.info("=" * 80)

        start_time = time.time()

        # 1. –û–±–Ω–æ–≤–∏—Ç—å —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫–∏ (–≤–æ—Ä–æ–Ω–∫–∏, —Å—Ç–∞–¥–∏–∏ –º–æ–≥—É—Ç –º–µ–Ω—è—Ç—å—Å—è)
        logger.info("üìö Updating dictionaries...")
        self.extract_dictionaries()

        # 2. –û–±–æ–≥–∞—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–º–∏ –ø–æ–ª—è–º–∏
        logger.info("üîß Enriching existing records with missing fields...")
        self.enrich_existing_records()

        # 3. –ó–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ HOURS_BACK —á–∞—Å–æ–≤
        logger.info(f"üì• Loading changes from last {HOURS_BACK} hours...")
        contacts_count = self.extract_contacts()
        deals_count = self.extract_deals()
        activities_count = self.extract_activities()

        duration = time.time() - start_time

        logger.info("=" * 80)
        logger.info("‚úÖ SMART INCREMENTAL SYNC COMPLETED")
        logger.info(f"   Duration: {duration:.2f}s")
        logger.info(f"   Contacts updated: {contacts_count}")
        logger.info(f"   Deals updated: {deals_count}")
        logger.info(f"   Activities updated: {activities_count}")
        logger.info("=" * 80)

    def enrich_existing_records(self):
        """–û–±–æ–≥–∞—Ç–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–∞–ø–∏—Å–∏ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–º–∏ –ø–æ–ª—è–º–∏"""
        try:
            # –û–±–æ–≥–∞—Ç–∏—Ç—å —Å–¥–µ–ª–∫–∏: –¥–æ–±–∞–≤–∏—Ç—å type_id –∏ category_id –≥–¥–µ NULL
            logger.info("  üîß Enriching deals with missing type_id and category_id...")

            # –ü–æ–ª—É—á–∏—Ç—å ID —Å–¥–µ–ª–æ–∫ –≥–¥–µ type_id –∏–ª–∏ category_id = NULL
            deals_to_enrich = self.supabase.table('deals')\
                .select('id')\
                .or_('type_id.is.null,category_id.is.null')\
                .limit(1000)\
                .execute()

            if deals_to_enrich.data and len(deals_to_enrich.data) > 0:
                deal_ids = [d['id'] for d in deals_to_enrich.data]
                logger.info(f"  Found {len(deal_ids)} deals to enrich")

                # –ó–∞–≥—Ä—É–∑–∏—Ç—å —ç—Ç–∏ —Å–¥–µ–ª–∫–∏ –∏–∑ –ë–∏—Ç—Ä–∏–∫—Å–∞ (–ø–æ –æ–¥–Ω–æ–π, —Ç.–∫. batch filter –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
                enriched_count = 0
                updates = []

                for deal_id in deal_ids:
                    try:
                        time.sleep(self.rate_limit_delay)
                        url = f"{self.bitrix_url}crm.deal.get.json"
                        response = requests.get(url, params={'id': deal_id}, timeout=30)
                        response.raise_for_status()
                        data = response.json()

                        if 'result' in data and data['result']:
                            deal = data['result']
                            deal_update = {
                                'id': self.safe_int(deal['ID']),
                                'type_id': deal.get('TYPE_ID') or None,
                                'category_id': self.safe_int(deal.get('CATEGORY_ID'))
                            }
                            updates.append(deal_update)
                            enriched_count += 1

                            # –í—Å—Ç–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏ –ø–æ 50
                            if len(updates) >= 50:
                                self.supabase.table('deals').upsert(updates).execute()
                                logger.info(f"  ‚úÖ Enriched {enriched_count}/{len(deal_ids)} deals")
                                updates = []

                    except Exception as e:
                        logger.warning(f"  ‚ö†Ô∏è  Failed to enrich deal {deal_id}: {e}")
                        continue

                # –í—Å—Ç–∞–≤–∏—Ç—å –æ—Å—Ç–∞—Ç–æ–∫
                if updates:
                    self.supabase.table('deals').upsert(updates).execute()
                    logger.info(f"  ‚úÖ Enriched {enriched_count}/{len(deal_ids)} deals (final batch)")
            else:
                logger.info("  ‚úÖ All deals already enriched")

        except Exception as e:
            logger.error(f"  ‚ùå Error enriching records: {e}")
            # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–∞–∂–µ –µ—Å–ª–∏ –æ–±–æ–≥–∞—â–µ–Ω–∏–µ –Ω–µ —É–¥–∞–ª–æ—Å—å


def main():
    """–¢–æ—á–∫–∞ –≤—Ö–æ–¥–∞"""
    logger.info("üöÄ Bitrix24 ETL Service initialized")
    logger.info(f"   Bitrix24: {BITRIX_WEBHOOK[:50]}...")
    logger.info(f"   Supabase: {SUPABASE_URL}")
    logger.info("")
    
    etl = Bitrix24ETL()
    
    if SYNC_MODE == 'full':
        etl.full_sync()
    elif SYNC_MODE == 'incremental':
        etl.incremental_sync()
    else:
        logger.error(f"‚ùå Unknown SYNC_MODE: {SYNC_MODE}")
        sys.exit(1)
    
    logger.info("üèÅ ETL process finished")


if __name__ == '__main__':
    main()